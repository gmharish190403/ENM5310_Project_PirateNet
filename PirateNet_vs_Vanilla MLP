import jax
import jax.numpy as jnp
from jax import random, grad, jit, vmap
import optax
import matplotlib.pyplot as plt
import time

P = 195.0
v = 800e-3
r0 = 52.5e-6
eta = 0.35
rho = 8440.0
cp = 450.0
k = 10.0
T_inf = 298.0
Tm = 1593.0

L_x = 1.5e-3
L_y = 0.75e-3
L_z = 0.5e-3

Pe = rho * cp * v * L_x / k
T_scale = eta * P / (k * L_x)

a = 1.5*r0
b = 2*r0
c_f = 2*r0
c_r = 4*r0
f_f = 0.6
f_r = 1.4

def goldak(xi, y, z):
    r2 = (y/a)**2 + (z/b)**2
    front = jnp.where(xi >= 0, 6*jnp.sqrt(3)*f_f/(a*b*c_f*jnp.pi*jnp.sqrt(jnp.pi)) * jnp.exp(-3*r2 - 3*(xi/c_f)**2), 0.0)
    rear = jnp.where(xi < 0, 6*jnp.sqrt(3)*f_r/(a*b*c_r*jnp.pi*jnp.sqrt(jnp.pi)) * jnp.exp(-3*r2 - 3*(xi/c_r)**2), 0.0)
    return (front + rear) / T_scale

class MLP:
    def __init__(self, key):
        layers = []
        dims = [3, 128, 128, 128, 128, 128, 128, 1]
        for i in range(len(dims)-1):
            key, k = random.split(key)
            w = random.normal(k, (dims[i], dims[i+1])) * jnp.sqrt(2.0/dims[i])
            b = jnp.zeros(dims[i+1])
            layers.append((w, b))
        self.params = layers
    
    def forward(self, x):
        for i, (w, b) in enumerate(self.params):
            x = x @ w + b
            if i < len(self.params) - 1:
                x = jnp.tanh(x)
        return x

class PirateNet:
    def __init__(self, key):
        key, k = random.split(key)
        w0 = random.normal(k, (3, 128)) * jnp.sqrt(2.0/3)
        b0 = jnp.zeros(128)
        self.input = (w0, b0)
        
        blocks = []
        for _ in range(6):
            key, k1, k2, k3 = random.split(key, 4)
            wu = random.normal(k1, (128, 128)) * jnp.sqrt(2.0/128)
            bu = jnp.zeros(128)
            wv = random.normal(k2, (128, 128)) * jnp.sqrt(2.0/128)
            bv = jnp.zeros(128)
            ww = random.normal(k3, (128, 128)) * jnp.sqrt(2.0/128)
            bw = jnp.zeros(128)
            blocks.append(((wu, bu), (wv, bv), (ww, bw)))
        self.blocks = blocks
        
        key, k = random.split(key)
        wf = random.normal(k, (128, 1)) * jnp.sqrt(2.0/128)
        bf = jnp.zeros(1)
        self.output = (wf, bf)
    
    def forward(self, x):
        w, b = self.input
        x = jnp.tanh(x @ w + b)
        
        for (wu, bu), (wv, bv), (ww, bw) in self.blocks:
            u = jnp.tanh(x @ wu + bu)
            v = jnp.tanh(x @ wv + bv)
            w = jnp.tanh(x @ ww + bw)
            x = x + u * v + (1 - v) * w
        
        w, b = self.output
        return x @ w + b

def normalize(xi, y, z):
    xi_n = 2*(xi + L_x/2)/L_x - 1
    y_n = 2*(y + L_y)/L_y - 1
    z_n = 2*z/L_z + 1
    return jnp.stack([xi_n, y_n, z_n], axis=-1)

def compute_loss(params, key, forward_fn):
    n = 10000
    xi = random.uniform(key, (n,), minval=-L_x/2, maxval=L_x/2)
    key, k = random.split(key)
    y = random.uniform(k, (n,), minval=-L_y, maxval=L_y)
    key, k = random.split(key)
    z = random.uniform(k, (n,), minval=-L_z, maxval=0)
    
    def T_fn(xi, y, z):
        X = normalize(xi, y, z)
        return forward_fn(params, X).squeeze()
    
    T = T_fn(xi, y, z)
    dxi = grad(T_fn, argnums=0)(xi, y, z)
    d2xi = grad(grad(T_fn, argnums=0), argnums=0)(xi, y, z)
    d2y = grad(grad(T_fn, argnums=1), argnums=1)(xi, y, z)
    d2z = grad(grad(T_fn, argnums=2), argnums=2)(xi, y, z)
    
    Q = goldak(xi, y, z)
    res = Pe * dxi - (d2xi + d2y + d2z) - Q
    loss_pde = jnp.mean(res**2)
    
    n_bc = 500
    key, k = random.split(key)
    xi_bc = random.uniform(k, (n_bc,), minval=-L_x/2, maxval=L_x/2)
    key, k = random.split(key)
    y_bc = random.uniform(k, (n_bc,), minval=-L_y, maxval=L_y)
    
    T_top = vmap(lambda x, y: T_fn(x, y, 0.0))(xi_bc, y_bc)
    loss_bc = jnp.mean(T_top**2)
    
    return loss_pde + 10.0 * loss_bc

def train_model(model, forward_fn):
    optimizer = optax.adam(1e-3)
    opt_state = optimizer.init(model)
    
    @jit
    def step(p, s, k):
        l, g = jax.value_and_grad(lambda x: compute_loss(x, k, forward_fn))(p)
        u, s = optimizer.update(g, s)
        p = optax.apply_updates(p, u)
        return p, s, l
    
    key = random.PRNGKey(42)
    losses = []
    t0 = time.time()
    
    for i in range(30000):
        key, k = random.split(key)
        model, opt_state, loss = step(model, opt_state, k)
        losses.append(float(loss))
        if i % 5000 == 0:
            print(f"{i}: {loss:.3e}")
    
    return model, losses, time.time() - t0

def eval_model(params, forward_fn):
    xi = jnp.linspace(-L_x/2, L_x/2, 100)
    y = jnp.linspace(-L_y, L_y, 100)
    z = jnp.linspace(-L_z, 0, 50)
    
    XI, Y = jnp.meshgrid(xi, y, indexing='ij')
    X_surf = normalize(XI.flatten(), Y.flatten(), jnp.zeros(XI.size))
    T_surf = vmap(lambda x: forward_fn(params, x).squeeze())(X_surf)
    T_surf = (T_surf.reshape(XI.shape) * T_scale + T_inf)
    
    XI, Z = jnp.meshgrid(xi, z, indexing='ij')
    X_xz = normalize(XI.flatten(), jnp.zeros(XI.size), Z.flatten())
    T_xz = vmap(lambda x: forward_fn(params, x).squeeze())(X_xz)
    T_xz = (T_xz.reshape(XI.shape) * T_scale + T_inf)
    
    depth = float(jnp.abs(z[jnp.max(jnp.where(T_xz[50, :] > Tm, jnp.arange(len(z)), 0))])) * 1e6
    width = float(2 * jnp.abs(y[jnp.max(jnp.where(T_surf[:, 0] > Tm, jnp.arange(len(y)), 0))])) * 1e6
    
    return T_surf, T_xz, xi, y, z, depth, width

print("Training MLP...")
mlp = MLP(random.PRNGKey(0))
mlp, mlp_loss, mlp_t = train_model(mlp.params, lambda p, x: MLP.__dict__['forward'](type('', (), {'params': p})(), x))

print("\nTraining PirateNet...")
pn = PirateNet(random.PRNGKey(0))
pn_params = (pn.input, pn.blocks, pn.output)
def pn_fwd(p, x):
    inp, blks, out = p
    w, b = inp
    x = jnp.tanh(x @ w + b)
    for (wu, bu), (wv, bv), (ww, bw) in blks:
        u = jnp.tanh(x @ wu + bu)
        v = jnp.tanh(x @ wv + bv)
        w = jnp.tanh(x @ ww + bw)
        x = x + u * v + (1 - v) * w
    w, b = out
    return x @ w + b

pn_params, pn_loss, pn_t = train_model(pn_params, pn_fwd)

print(f"\nMLP: {mlp_loss[-1]:.2e}, {mlp_t:.0f}s")
print(f"PirateNet: {pn_loss[-1]:.2e}, {pn_t:.0f}s")

mlp_surf, mlp_xz, xi, y, z, mlp_d, mlp_w = eval_model(mlp, lambda p, x: MLP.__dict__['forward'](type('', (), {'params': p})(), x))
pn_surf, pn_xz, xi, y, z, pn_d, pn_w = eval_model(pn_params, pn_fwd)

print(f"MLP depth: {mlp_d:.1f} um, width: {mlp_w:.1f} um")
print(f"PirateNet depth: {pn_d:.1f} um, width: {pn_w:.1f} um")

fig, ax = plt.subplots(2, 3, figsize=(14, 9))

ax[0,0].contourf(xi*1e3, z*1e6, mlp_xz.T, 20)
ax[0,0].contour(xi*1e3, z*1e6, mlp_xz.T, [Tm], colors='r')
ax[0,0].set_title('MLP XZ')

ax[1,0].contourf(xi*1e3, z*1e6, pn_xz.T, 20)
ax[1,0].contour(xi*1e3, z*1e6, pn_xz.T, [Tm], colors='r')
ax[1,0].set_title('PirateNet XZ')

ax[0,1].contourf(xi*1e3, y*1e3, mlp_surf, 20)
ax[0,1].contour(xi*1e3, y*1e3, mlp_surf, [Tm], colors='r')
ax[0,1].set_title('MLP Surface')

ax[1,1].contourf(xi*1e3, y*1e3, pn_surf, 20)
ax[1,1].contour(xi*1e3, y*1e3, pn_surf, [Tm], colors='r')
ax[1,1].set_title('PirateNet Surface')

ax[0,2].plot(mlp_loss, label='MLP')
ax[0,2].plot(pn_loss, label='PirateNet')
ax[0,2].set_yscale('log')
ax[0,2].legend()

ax[1,2].text(0.1, 0.7, f"MLP\n{mlp_loss[-1]:.2e}\n{mlp_t:.0f}s\n{mlp_d:.0f}um", fontsize=11)
ax[1,2].text(0.1, 0.3, f"PirateNet\n{pn_loss[-1]:.2e}\n{pn_t:.0f}s\n{pn_d:.0f}um", fontsize=11)
ax[1,2].axis('off')

plt.tight_layout()
plt.savefig('results.png')
plt.show()
