import jax
import jax.numpy as jnp
from jax import random, grad, jit, vmap
import optax
import matplotlib.pyplot as plt
import time

P = 195.0
v = 800e-3
r0 = 52.5e-6
eta = 0.35
rho = 8440.0
cp = 450.0
k = 10.0
T_inf = 298.0
Tm = 1593.0

L_x = 1.5e-3
L_y = 0.75e-3
L_z = 0.5e-3

Pe = rho * cp * v * L_x / k
T_scale = eta * P / (k * L_x)

a = 1.5*r0
b = 2*r0
c_f = 2*r0
c_r = 4*r0
f_f = 0.6
f_r = 1.4

def goldak(xi, y, z):
    r2 = (y/a)**2 + (z/b)**2
    front = jnp.where(xi >= 0, 6*jnp.sqrt(3)*f_f/(a*b*c_f*jnp.pi*jnp.sqrt(jnp.pi)) * jnp.exp(-3*r2 - 3*(xi/c_f)**2), 0.0)
    rear = jnp.where(xi < 0, 6*jnp.sqrt(3)*f_r/(a*b*c_r*jnp.pi*jnp.sqrt(jnp.pi)) * jnp.exp(-3*r2 - 3*(xi/c_r)**2), 0.0)
    return (front + rear) / T_scale

class PirateNet:
    def __init__(self, key):
        key, k = random.split(key)
        w0 = random.normal(k, (3, 128)) * jnp.sqrt(2.0/3)
        b0 = jnp.zeros(128)
        self.input = (w0, b0)
        
        blocks = []
        for _ in range(6):
            key, k1, k2, k3 = random.split(key, 4)
            wu = random.normal(k1, (128, 128)) * jnp.sqrt(2.0/128)
            bu = jnp.zeros(128)
            wv = random.normal(k2, (128, 128)) * jnp.sqrt(2.0/128)
            bv = jnp.zeros(128)
            ww = random.normal(k3, (128, 128)) * jnp.sqrt(2.0/128)
            bw = jnp.zeros(128)
            blocks.append(((wu, bu), (wv, bv), (ww, bw)))
        self.blocks = blocks
        
        key, k = random.split(key)
        wf = random.normal(k, (128, 1)) * jnp.sqrt(2.0/128)
        bf = jnp.zeros(1)
        self.output = (wf, bf)
    
    def forward(self, x):
        w, b = self.input
        x = jnp.tanh(x @ w + b)
        
        for (wu, bu), (wv, bv), (ww, bw) in self.blocks:
            u = jnp.tanh(x @ wu + bu)
            v = jnp.tanh(x @ wv + bv)
            w = jnp.tanh(x @ ww + bw)
            x = x + u * v + (1 - v) * w
        
        w, b = self.output
        return x @ w + b

def normalize(xi, y, z):
    xi_n = 2*(xi + L_x/2)/L_x - 1
    y_n = 2*(y + L_y)/L_y - 1
    z_n = 2*z/L_z + 1
    return jnp.stack([xi_n, y_n, z_n], axis=-1)

def pn_fwd(p, x):
    inp, blks, out = p
    w, b = inp
    x = jnp.tanh(x @ w + b)
    for (wu, bu), (wv, bv), (ww, bw) in blks:
        u = jnp.tanh(x @ wu + bu)
        v = jnp.tanh(x @ wv + bv)
        w = jnp.tanh(x @ ww + bw)
        x = x + u * v + (1 - v) * w
    w, b = out
    return x @ w + b

def sample_uniform(key, n):
    xi = random.uniform(key, (n,), minval=-L_x/2, maxval=L_x/2)
    key, k = random.split(key)
    y = random.uniform(k, (n,), minval=-L_y, maxval=L_y)
    key, k = random.split(key)
    z = random.uniform(k, (n,), minval=-L_z, maxval=0)
    return xi, y, z

def sample_importance(key, n):
    n_laser = int(0.5 * n)
    n_bulk = n - n_laser
    
    xi_l = random.uniform(key, (n_laser,), minval=-3*c_r, maxval=3*c_f)
    key, k = random.split(key)
    y_l = random.uniform(k, (n_laser,), minval=-3*a, maxval=3*a)
    key, k = random.split(key)
    z_l = random.uniform(k, (n_laser,), minval=-3*b, maxval=0)
    
    key, k = random.split(key)
    xi_b = random.uniform(k, (n_bulk,), minval=-L_x/2, maxval=L_x/2)
    key, k = random.split(key)
    y_b = random.uniform(k, (n_bulk,), minval=-L_y, maxval=L_y)
    key, k = random.split(key)
    z_b = random.uniform(k, (n_bulk,), minval=-L_z, maxval=0)
    
    xi = jnp.concatenate([xi_l, xi_b])
    y = jnp.concatenate([y_l, y_b])
    z = jnp.concatenate([z_l, z_b])
    return xi, y, z

def compute_residuals(params, xi, y, z):
    def T_fn(xi, y, z):
        X = normalize(xi, y, z)
        return pn_fwd(params, X).squeeze()
    
    dxi = grad(T_fn, argnums=0)(xi, y, z)
    d2xi = grad(grad(T_fn, argnums=0), argnums=0)(xi, y, z)
    d2y = grad(grad(T_fn, argnums=1), argnums=1)(xi, y, z)
    d2z = grad(grad(T_fn, argnums=2), argnums=2)(xi, y, z)
    
    Q = goldak(xi, y, z)
    return Pe * dxi - (d2xi + d2y + d2z) - Q

def compute_loss(params, key, sample_fn, residuals=None):
    n = 10000
    xi, y, z = sample_fn(key, n)
    
    res = vmap(lambda x, y, z: compute_residuals(params, x, y, z))(xi, y, z)
    loss_pde = jnp.mean(res**2)
    
    n_bc = 500
    key, k = random.split(key)
    xi_bc = random.uniform(k, (n_bc,), minval=-L_x/2, maxval=L_x/2)
    key, k = random.split(key)
    y_bc = random.uniform(k, (n_bc,), minval=-L_y, maxval=L_y)
    
    X_bc = normalize(xi_bc, y_bc, jnp.zeros(n_bc))
    T_top = vmap(lambda x: pn_fwd(params, x).squeeze())(X_bc)
    loss_bc = jnp.mean(T_top**2)
    
    return loss_pde + 10.0 * loss_bc, res

def train_uniform(model):
    optimizer = optax.adam(1e-3)
    opt_state = optimizer.init(model)
    
    @jit
    def step(p, s, k):
        l, g = jax.value_and_grad(lambda x: compute_loss(x, k, sample_uniform)[0])(p)
        u, s = optimizer.update(g, s)
        p = optax.apply_updates(p, u)
        return p, s, l
    
    key = random.PRNGKey(42)
    losses = []
    t0 = time.time()
    
    for i in range(20000):
        key, k = random.split(key)
        model, opt_state, loss = step(model, opt_state, k)
        losses.append(float(loss))
        if i % 5000 == 0:
            print(f"{i}: {loss:.3e}")
    
    return model, losses, time.time() - t0

def train_importance(model):
    optimizer = optax.adam(1e-3)
    opt_state = optimizer.init(model)
    
    @jit
    def step(p, s, k):
        l, g = jax.value_and_grad(lambda x: compute_loss(x, k, sample_importance)[0])(p)
        u, s = optimizer.update(g, s)
        p = optax.apply_updates(p, u)
        return p, s, l
    
    key = random.PRNGKey(42)
    losses = []
    t0 = time.time()
    
    for i in range(20000):
        key, k = random.split(key)
        model, opt_state, loss = step(model, opt_state, k)
        losses.append(float(loss))
        if i % 5000 == 0:
            print(f"{i}: {loss:.3e}")
    
    return model, losses, time.time() - t0

def train_rar(model):
    optimizer = optax.adam(1e-3)
    opt_state = optimizer.init(model)
    
    key = random.PRNGKey(42)
    losses = []
    t0 = time.time()
    
    k_resample = 1000
    pool_size = 50000
    
    key, k = random.split(key)
    xi_pool, y_pool, z_pool = sample_uniform(k, pool_size)
    
    @jit
    def step(p, s, xi, y, z):
        res = vmap(lambda x, y, z: compute_residuals(p, x, y, z))(xi, y, z)
        
        k_tmp = random.PRNGKey(0)
        prob = jnp.abs(res) / jnp.sum(jnp.abs(res))
        idx = random.choice(k_tmp, pool_size, shape=(10000,), p=prob)
        
        xi_s = xi[idx]
        y_s = y[idx]
        z_s = z[idx]
        
        def loss_fn(params):
            res_s = vmap(lambda x, y, z: compute_residuals(params, x, y, z))(xi_s, y_s, z_s)
            loss_pde = jnp.mean(res_s**2)
            
            n_bc = 500
            k_bc = random.PRNGKey(1)
            xi_bc = random.uniform(k_bc, (n_bc,), minval=-L_x/2, maxval=L_x/2)
            k_bc, k2 = random.split(k_bc)
            y_bc = random.uniform(k2, (n_bc,), minval=-L_y, maxval=L_y)
            
            X_bc = normalize(xi_bc, y_bc, jnp.zeros(n_bc))
            T_top = vmap(lambda x: pn_fwd(params, x).squeeze())(X_bc)
            loss_bc = jnp.mean(T_top**2)
            
            return loss_pde + 10.0 * loss_bc
        
        l, g = jax.value_and_grad(loss_fn)(p)
        u, s = optimizer.update(g, s)
        p = optax.apply_updates(p, u)
        return p, s, l
    
    step_jit = jit(step)
    
    for i in range(20000):
        if i % k_resample == 0 and i > 0:
            key, k = random.split(key)
            xi_pool, y_pool, z_pool = sample_uniform(k, pool_size)
        
        model, opt_state, loss = step_jit(model, opt_state, xi_pool, y_pool, z_pool)
        losses.append(float(loss))
        if i % 5000 == 0:
            print(f"{i}: {loss:.3e}")
    
    return model, losses, time.time() - t0

def eval_model(params):
    xi = jnp.linspace(-L_x/2, L_x/2, 100)
    z = jnp.linspace(-L_z, 0, 50)
    
    XI, Z = jnp.meshgrid(xi, z, indexing='ij')
    X_xz = normalize(XI.flatten(), jnp.zeros(XI.size), Z.flatten())
    T_xz = vmap(lambda x: pn_fwd(params, x).squeeze())(X_xz)
    T_xz = (T_xz.reshape(XI.shape) * T_scale + T_inf)
    
    depth = float(jnp.abs(z[jnp.max(jnp.where(T_xz[50, :] > Tm, jnp.arange(len(z)), 0))])) * 1e6
    return T_xz, xi, z, depth

print("Training with Uniform Sampling...")
pn1 = PirateNet(random.PRNGKey(0))
params1 = (pn1.input, pn1.blocks, pn1.output)
params1, loss1, t1 = train_uniform(params1)

print("\nTraining with Importance Sampling...")
pn2 = PirateNet(random.PRNGKey(0))
params2 = (pn2.input, pn2.blocks, pn2.output)
params2, loss2, t2 = train_importance(params2)

print("\nTraining with RAR...")
pn3 = PirateNet(random.PRNGKey(0))
params3 = (pn3.input, pn3.blocks, pn3.output)
params3, loss3, t3 = train_rar(params3)

T1, xi, z, d1 = eval_model(params1)
T2, xi, z, d2 = eval_model(params2)
T3, xi, z, d3 = eval_model(params3)

print(f"\nUniform: {loss1[-1]:.2e}, {t1:.0f}s, {d1:.1f}um")
print(f"Importance: {loss2[-1]:.2e}, {t2:.0f}s, {d2:.1f}um")
print(f"RAR: {loss3[-1]:.2e}, {t3:.0f}s, {d3:.1f}um")

fig, ax = plt.subplots(2, 2, figsize=(12, 10))

ax[0,0].contourf(xi*1e3, z*1e6, T1.T, 20)
ax[0,0].contour(xi*1e3, z*1e6, T1.T, [Tm], colors='r')
ax[0,0].set_title('Uniform')

ax[0,1].contourf(xi*1e3, z*1e6, T2.T, 20)
ax[0,1].contour(xi*1e3, z*1e6, T2.T, [Tm], colors='r')
ax[0,1].set_title('Importance')

ax[1,0].contourf(xi*1e3, z*1e6, T3.T, 20)
ax[1,0].contour(xi*1e3, z*1e6, T3.T, [Tm], colors='r')
ax[1,0].set_title('RAR')

ax[1,1].plot(loss1, label='Uniform')
ax[1,1].plot(loss2, label='Importance')
ax[1,1].plot(loss3, label='RAR')
ax[1,1].set_yscale('log')
ax[1,1].legend()
ax[1,1].set_xlabel('Epoch')
ax[1,1].set_ylabel('Loss')

plt.tight_layout()
plt.savefig('sampling_comparison.png')
plt.show()
